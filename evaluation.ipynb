{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "import librosa\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Must match training configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PRETRAINED_MODEL_NAME = \"openai/whisper-large-v3\"\n",
    "PRETRAINED_MODEL_OUTPUT_DIM = 1280\n",
    "PRETRAINED_MAX_LEN_PADDING = 448\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# Paths for saved models and features\n",
    "MODEL_DIR = Path('./model')\n",
    "FEATURE_DIR = Path('./model_features')\n",
    "BEST_MODEL_PATH = MODEL_DIR / f'best_lstm_with_{PRETRAINED_MODEL_NAME.split(\"/\")[-1]}_features.pth'\n",
    "LABEL_ENCODER_PATH = MODEL_DIR / 'label_encoder.joblib'\n",
    "FEATURES_METADATA_PATH = FEATURE_DIR / 'features_metadata.csv'\n",
    "\n",
    "# LSTM Model Parameters (must match training)\n",
    "LSTM_HIDDEN_SIZE = 256\n",
    "LSTM_NUM_LAYERS = 3\n",
    "LSTM_DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture (Re-define as in serv9.ipynb)\n",
    "class EmotionClassifierLSTM(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, n_layers: int, num_classes: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.lstm_layer = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_prob if n_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.classifier_hidden_layer = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.relu_activation = nn.ReLU()\n",
    "        self.dropout_layer = nn.Dropout(dropout_prob)\n",
    "        self.output_layer = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_features: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = input_features.size(0)\n",
    "        num_directions = 2\n",
    "        h0 = torch.zeros(self.lstm_layer.num_layers * num_directions, batch_size, self.lstm_layer.hidden_size).to(input_features.device)\n",
    "        c0 = torch.zeros(self.lstm_layer.num_layers * num_directions, batch_size, self.lstm_layer.hidden_size).to(input_features.device)\n",
    "        lstm_output, _ = self.lstm_layer(input_features, (h0, c0))\n",
    "        last_step_output = lstm_output[:, -1, :]\n",
    "        x = self.classifier_hidden_layer(last_step_output)\n",
    "        x = self.relu_activation(x)\n",
    "        x = self.dropout_layer(x)\n",
    "        logits = self.output_layer(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class (Re-define as in serv9.ipynb)\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_df: pd.DataFrame, \n",
    "                 feature_col: str, \n",
    "                 label_col: str, \n",
    "                 label_encoder: LabelEncoder):\n",
    "        self.data_df = data_df\n",
    "        self.feature_col = feature_col\n",
    "        self.label_col = label_col\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        data_row = self.data_df.iloc[index]\n",
    "        feature_path = data_row[self.feature_col]\n",
    "        label_text = data_row[self.label_col]\n",
    "\n",
    "        feature_tensor = torch.load(feature_path)\n",
    "        label_id = self.label_encoder.transform([label_text])[0]\n",
    "        label_tensor = torch.tensor(label_id, dtype=torch.long)\n",
    "        \n",
    "        return feature_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and prepare for evaluation\n",
    "print(f\"Loading features metadata from: {FEATURES_METADATA_PATH}\")\n",
    "df_features = pd.read_csv(FEATURES_METADATA_PATH)\n",
    "\n",
    "print(f\"Loading label encoder from: {LABEL_ENCODER_PATH}\")\n",
    "label_encoder = joblib.load(LABEL_ENCODER_PATH)\n",
    "NUM_CLASSES = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"Mapping classes: {dict(zip(label_encoder.classes_, range(NUM_CLASSES)))}\")\n",
    "\n",
    "# Re-split data to get the test set (must match training split)\n",
    "df_features['encoded_labels'] = label_encoder.transform(df_features['labels'])\n",
    "_, val_test_df = train_test_split(df_features, test_size=0.30, stratify=df_features['encoded_labels'], random_state=42)\n",
    "_, test_df = train_test_split(val_test_df, test_size=0.50, stratify=val_test_df['encoded_labels'], random_state=42)\n",
    "\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "# Create test dataset and data loader\n",
    "test_dataset = EmotionDataset(\n",
    "    data_df=test_df,\n",
    "    feature_col=\"feature_path\",\n",
    "    label_col=\"labels\",\n",
    "    label_encoder=label_encoder,\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 16 # Must match training batch size\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and load the best model\n",
    "print(f\"Initializing model and loading weights from: {BEST_MODEL_PATH}\")\n",
    "model = EmotionClassifierLSTM(\n",
    "    input_dim=PRETRAINED_MODEL_OUTPUT_DIM,\n",
    "    hidden_dim=LSTM_HIDDEN_SIZE,\n",
    "    n_layers=LSTM_NUM_LAYERS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    dropout_prob=LSTM_DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=DEVICE))\n",
    "model.eval() # Set to evaluation mode\n",
    "\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get predictions (from serv9.ipynb)\n",
    "def get_predictions(model, data_loader, device):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds.extend(torch.max(outputs, 1)[1].cpu().numpy())\n",
    "            labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    return np.array(preds), np.array(labels)\n",
    "\n",
    "# Get predictions from the test set\n",
    "print(\"Getting predictions on the test set...\")\n",
    "y_pred, y_true = get_predictions(model, test_loader, DEVICE)\n",
    "\n",
    "class_names = label_encoder.classes_\n",
    "print(\"Predictions obtained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Confusion Matrix\n",
    "print(\"Generating Confusion Matrix...\")\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.show()\n",
    "print(\"Confusion Matrix displayed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"Generating Classification Report...\")\n",
    "print(\"\\nClassification Report - Test Set:\")\n",
    "try:\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, zero_division=0))\n",
    "except ValueError:\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "    print(f\"Mapping ID to class names: {dict(enumerate(class_names))}\")\n",
    "print(\"Classification Report displayed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check with Test Data (Optional, from serv9.ipynb)\n",
    "def run_sanity_checks(current_model, test_df, label_encoder, device, num_checks=20):\n",
    "    current_model.eval()  # Mode evaluasi\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    print(\"--- Starting Sanity Check ---\\n\")\n",
    "    total_samples = len(test_df)\n",
    "    sample_size = min(num_checks, total_samples)\n",
    "    indices = random.sample(range(total_samples), sample_size)\n",
    "\n",
    "    for idx, df_idx in enumerate(indices):\n",
    "        row = test_df.iloc[df_idx]\n",
    "        feature_path = row['feature_path']\n",
    "        true_label = row['labels']\n",
    "\n",
    "        # Load features and predict\n",
    "        with torch.no_grad():\n",
    "            feature_tensor = torch.load(feature_path, map_location=device).unsqueeze(0)\n",
    "            output = current_model(feature_tensor)\n",
    "            _, predicted_id = torch.max(output, 1)\n",
    "            predicted_label = label_encoder.inverse_transform(predicted_id.cpu().numpy())[0]\n",
    "\n",
    "        # Display results\n",
    "        original_audio_path = row.get('path', 'N/A')\n",
    "        print(f\"Sample {idx+1}/{sample_size}:\")\n",
    "        print(f\"   Original Audio   : {original_audio_path}\")\n",
    "        print(f\"   Feature File     : {feature_path}\")\n",
    "        print(f\"   True Label       : {true_label}\")\n",
    "        print(f\"   Predicted Label  : {predicted_label}\")\n",
    "        result = \"Correct\" if true_label == predicted_label else \"INCORRECT\"\n",
    "        print(f\"   Result           : {result}\\n\")\n",
    "\n",
    "        if true_label == predicted_label:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    # Final summary\n",
    "    accuracy = (correct_predictions / sample_size) * 100 if sample_size > 0 else 0\n",
    "    print(\"--- Sanity Check Summary ---\")\n",
    "    print(f\"Total samples checked : {sample_size}\")\n",
    "    print(f\"Correct predictions   : {correct_predictions}\")\n",
    "    print(f\"Accuracy              : {accuracy:.2f}%\\n\")\n",
    "    print(\"--- Sanity Check Complete ---\")\n",
    "\n",
    "NUM_SANITY_CHECKS = 20 # Number of random samples to check\n",
    "run_sanity_checks(model, test_df, label_encoder, DEVICE, NUM_SANITY_CHECKS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}